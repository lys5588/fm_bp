{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc68032-7afa-4f79-bfcc-13fcc9420e73",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0df0d6-0908-47c9-b0e6-4cecc3fc225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4e8189-8d75-49ea-9cbc-8015334a3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_to_tensor(directory):\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensors = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            image = Image.open(image_path)\n",
    "            image_tensor = transform(image)\n",
    "            image_tensors.append(image_tensor)\n",
    "    return torch.stack(image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5a486f8-e522-4a29-a62c-9126abcd5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "images= read_images_to_tensor(\"./data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51dfd45-cdba-42d8-8163-8fd92989cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1064, 1900])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0972831e-8a2c-4153-9aca-acc1cfe3c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertical_gradient(images_tensor):\n",
    "    kernel = (\n",
    "        torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    gradient = torch.nn.functional.conv2d(images_tensor, kernel, padding=1)\n",
    "    gradient_image = gradient.abs().sum(dim=1, keepdim=True)\n",
    "    return gradient, gradient_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ca8ad8-7392-4176-8d75-2dc5d28bd8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 1, 3, 3], expected input[1, 4, 1064, 1900] to have 1 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ver_grad,ver_grad_img \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_vertical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mcompute_vertical_gradient\u001b[1;34m(images_tensor)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_vertical_gradient\u001b[39m(images_tensor):\n\u001b[0;32m      2\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      3\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     )\n\u001b[1;32m----> 7\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     gradient_image \u001b[38;5;241m=\u001b[39m gradient\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradient, gradient_image\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 1, 3, 3], expected input[1, 4, 1064, 1900] to have 1 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "ver_grad,ver_grad_img = compute_vertical_gradient(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038eaa5e-1290-4626-8264-2d4b03cdb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vertical_gradient(images_tensor):\n",
    "    channels = images_tensor.shape[1]\n",
    "    kernel = (\n",
    "        torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    kernel = kernel.repeat( 1,channels, 1, 1)  # Repeat the kernel for each channel\n",
    "    gradient = torch.nn.functional.conv2d(images_tensor, kernel, padding=1)\n",
    "    gradient_image = gradient.abs().sum(dim=1, keepdim=True)\n",
    "    return gradient, gradient_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b56fe49-8d8e-464b-9eec-d54c1c3fd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_grad,ver_grad_img = compute_vertical_gradient(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc984bb-a132-4930-bd91-ad6842cc593b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1064, 1900])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ver_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5753c9f6-8d7f-4fbd-993a-2d5462e6d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1064, 1900])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ver_grad_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faff3094-7458-40e8-9561-0010598bba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(tensor, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    to_pil = transforms.ToPILImage()\n",
    "    for i, img_tensor in enumerate(tensor):\n",
    "        img = to_pil(img_tensor)\n",
    "        img.save(os.path.join(directory, f\"image_{i}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc7d7f2-f787-4c76-ae99-94187a450d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(ver_grad_img,\"./fig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56464279-3cbf-462f-8b7f-6278ae357a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_horizontal_gradient(images_tensor):\n",
    "    channels = images_tensor.shape[1]\n",
    "    kernel = (\n",
    "        torch.tensor([[1, 1, 1], [0, 0, 0], [-1, -1, -1]], dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "    )\n",
    "    kernel = kernel.repeat(1, channels, 1, 1)  # Repeat the kernel for each channel\n",
    "    gradient = torch.nn.functional.conv2d(images_tensor, kernel, padding=1)\n",
    "    gradient_image = gradient.abs().sum(dim=1, keepdim=True)\n",
    "    return gradient, gradient_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09350964-0c03-4eb2-bb0b-b1ed6082fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_grad,ver_grad_img = compute_horizontal_gradient(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2226659-2c89-4ef5-8eae-e26358a3aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(ver_grad_img,\"./fig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e680e6c-d451-4e84-a642-f1ef1bda5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_grad,ver_grad_img = compute_vertical_gradient(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da4095c5-d472-41ee-9046-2712f134361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_grad,hor_grad_img = compute_horizontal_gradient(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc125e80-89cf-415f-9704-c4fb8f05d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_img=torch.cat([ver_grad_img,hor_grad_img],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a22a71dd-3311-4fbc-bf98-312f02060958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1064, 1900])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d85f62f-9dc0-4495-950c-9a794544b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(grad_img,\"./fig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41c9ccd7-6fe8-4908-b271-6b0656125fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6d3c50-58b0-46a0-9618-377e5c6af1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=unpickle(\"./data/cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffb55c1f-a1be-4f22-a9a3-33d03ed265cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d611c4e4-220c-4609-a105-229b4d832e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cb252ed-4f72-4310-9b4a-e4bb950c693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = data[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32e2e1fa-5b4c-449e-900c-d25d7d9cb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(image_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1785c60f-016d-4b1f-9e46-22f739242db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3072])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c4311c5-6335-4c46-800d-3a21a800d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor=data_tensor.reshape(data_tensor.shape[0],3,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff44d8db-e4b2-452c-8583-a2dedfe80b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 32, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "998c0cdb-e1f8-41bb-998b-8921ca6ec8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(data_tensor,\"./fig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f24d5190-1fc0-489e-8ecb-570fecfcb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d83dc897-e8e9-43ec-8f06-d6877049184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60d60e3c-92d2-4a94-b819-179c78ae5fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22e3156b-9197-44c4-8c7e-1ada4fd7bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379e6350-695a-4bd7-b410-792af437cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar10_to_tensor(directory):\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "\n",
    "        with open(file, \"rb\") as fo:\n",
    "            dict = pickle.load(fo, encoding=\"bytes\")\n",
    "        return dict\n",
    "\n",
    "    def extract_data_and_labels(batch):\n",
    "        data = batch[b\"data\"]\n",
    "        labels = batch[b\"labels\"]\n",
    "        return data, labels\n",
    "\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"data_batch\"):\n",
    "            data = unpickle(os.path.join(directory, filename))\n",
    "            images, labels = extract_data_and_labels(data)\n",
    "            images = torch.tensor(images, dtype=torch.float32).reshape(-1, 3, 32, 32)\n",
    "            labels = torch.tensor(labels)\n",
    "            all_images.append(images)\n",
    "            all_labels.append(labels)\n",
    "        if filename.startswith(\"test_batch\"):\n",
    "            data = unpickle(os.path.join(directory, filename))\n",
    "            images, labels = extract_data_and_labels(data)\n",
    "            images = torch.tensor(images, dtype=torch.float32).reshape(-1, 3, 32, 32)\n",
    "            labels = torch.tensor(labels)\n",
    "            test_images.append(images)\n",
    "            test_labels.append(labels)\n",
    "    all_images = torch.cat(all_images, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    test_images = test_images[0]\n",
    "    test_labels = test_labels[0]\n",
    "    return all_images, all_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518b93c2-14ec-4240-b460-6abc23fb206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels,test_images,test_labels = read_cifar10_to_tensor(\"./data/cifar-10-batches-py/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b4b719-acff-4dc6-bcb8-4f86d4982e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5cafbd-6dbc-42f4-a7b2-3625454c0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSiftFeature(img):\n",
    "    # 设置图像sift特征关键点最大为200\n",
    "    sift = cv2.SIFT_create()\n",
    "    # 计算图片的特征点和特征点描述\n",
    "    # Ensure the input tensor is on CPU and convert to numpy array\n",
    "    img = img.cpu().numpy()\n",
    "\n",
    "    # Initialize SIFT detector\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(\"uint8\")\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # print(img.shape)\n",
    "    # break\n",
    "\n",
    "    # Detect SIFT features\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7d8250-3225-461c-8fd4-8712078deae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算词袋\n",
    "def learnVocabulary(features):\n",
    "    wordCnt = 50\n",
    "    # criteria表示迭代停止的模式   eps---精度0.1，max_iter---满足超过最大迭代次数20\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 0.1)\n",
    "    # 得到k-means聚类的初始中心点\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    # 标签，中心 = kmeans(输入数据（特征)、聚类的个数K,预设标签，聚类停止条件、重复聚类次数、初始聚类中心点\n",
    "    compactness, labels, centers = cv2.kmeans(\n",
    "        features, wordCnt, None, criteria, 20, flags\n",
    "    )\n",
    "    return centers\n",
    "\n",
    "\n",
    "# 计算特征向量\n",
    "def calcFeatVec(features, centers):\n",
    "    featVec = np.zeros((1, 50))\n",
    "    for i in range(0, features.shape[0]):\n",
    "        # 第i张图片的特征点\n",
    "        fi = features[i]\n",
    "        diffMat = np.tile(fi, (50, 1)) - centers\n",
    "        # axis=1按行求和，即求特征到每个中心点的距离\n",
    "        sqSum = (diffMat**2).sum(axis=1)\n",
    "        dist = sqSum**0.5\n",
    "        # 升序排序\n",
    "        sortedIndices = dist.argsort()\n",
    "        # 取出最小的距离，即找到最近的中心点\n",
    "        idx = sortedIndices[0]\n",
    "        # 该中心点对应+1\n",
    "        featVec[0][idx] += 1\n",
    "    return featVec\n",
    "\n",
    "\n",
    "# 建立词袋\n",
    "def build_center(images):\n",
    "    features = np.float32([]).reshape(0, 128)\n",
    "    for idx in range(images.shape[0]):\n",
    "        img = images[idx]\n",
    "        # 获取图片sift特征点\n",
    "        img_f = calcSiftFeature(img)\n",
    "        # 特征点加入训练数据\n",
    "        # print(img_f.shape)\n",
    "        # break\n",
    "        if img_f is None:\n",
    "            continue\n",
    "        features = np.append(features, img_f, axis=0)\n",
    "    # 训练集的词袋\n",
    "    centers = learnVocabulary(features)\n",
    "    # #将词袋保存\n",
    "    filename = \"./svm_centers.npy\"\n",
    "    np.save(filename, centers)\n",
    "    print(\"词袋:\", centers.shape)\n",
    "    return centers\n",
    "\n",
    "\n",
    "# 计算训练集图片特征向量\n",
    "def cal_vec(images):\n",
    "    centers = np.load(\"./svm_centers.npy\")\n",
    "    data_vec = np.float32([]).reshape(0, 50)  # 存放训练集图片的特征\n",
    "    labels = np.float32([])\n",
    "    # cate=[path+'/'+x for x in os.listdir(path) if os.path.isdir(path+'/'+x)]\n",
    "    for idx in range(images.shape[0]):\n",
    "        # 获取图片sift特征点\n",
    "        # print(idx)\n",
    "        img_f = calcSiftFeature(images[idx])\n",
    "        if img_f is None:\n",
    "            continue\n",
    "        img_vec = calcFeatVec(img_f, centers)\n",
    "        data_vec = np.append(data_vec, img_vec, axis=0)\n",
    "        labels = np.append(labels, idx)\n",
    "    print(\"data_vec:\", data_vec.shape)\n",
    "    print(\"image features vector done!\")\n",
    "    return data_vec, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e054afb2-f7d5-4911-a527-17cbdd6cd7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词袋: (50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[16.843645 , 15.270925 , 15.28492  , ...,  4.868086 ,  4.606535 ,\n",
       "         6.266907 ],\n",
       "       [41.428413 , 24.268282 , 16.101927 , ...,  5.388106 ,  4.769053 ,\n",
       "         6.2546806],\n",
       "       [23.229614 , 49.211975 , 43.35914  , ..., 13.342103 , 14.902228 ,\n",
       "        18.714949 ],\n",
       "       ...,\n",
       "       [18.13149  , 52.194286 , 88.70355  , ..., 10.448127 , 11.786611 ,\n",
       "        12.380846 ],\n",
       "       [16.309874 , 16.54392  , 18.711662 , ...,  3.780453 ,  3.5309558,\n",
       "         8.99455  ],\n",
       "       [ 8.055738 ,  6.537564 ,  8.386214 , ...,  4.802091 ,  5.3674326,\n",
       "         7.7283006]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_center(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b08f5d5a-7838-44b6-8163-bb2d1162610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_vec: (49945, 50)\n",
      "image features vector done!\n"
     ]
    }
   ],
   "source": [
    "data_vec,labels = cal_vec(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ef55181-6290-4ccd-a177-29e1c0bfcbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09ba8dbf-8a64-4ca9-a77d-2d38baaf23d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49945, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64e00d1b-3512-4b77-8e3a-0f54ae1a6297",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [49945, 50000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train the svm model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mSVM_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mSVM_Train\u001b[1;34m(data_vec, labels)\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(decision_function_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#利用x_train,y_train训练SVM分类器，获得参数\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(clf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me:/flowers/svm/svm_model.m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\utils\\validation.py:1320\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1320\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [49945, 50000]"
     ]
    }
   ],
   "source": [
    "# train the svm model\n",
    "SVM_Train(data_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5794ca00-e531-4c59-beeb-3f3c31f92894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f539b7-4201-4419-81bf-e1ea5ab2a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0967b0ac-be44-431b-a100-cde9c39a9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0677486-28c6-4433-ac32-0a891dd1f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf6eb18-cb47-4eca-a777-88d69c426c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47656b72-ceb4-49e7-bd52-5ca28a83cd80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f09279-b64d-4569-9478-2453223b09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in trainloader:\n",
    "    a=d\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faded93f-955b-4319-9fb8-0839d95d938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db03579d-a048-493a-8eb0-212735f13a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0211d2cf-5164-4e60-abfa-9e176538817d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdf6a5d6-0006-4c58-9bb2-ec7f58fc6e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848ce7fc-91ee-4fc3-954b-8e6d9a44680d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "trainset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d410a156-9fc2-42b0-9857-bb24f21d68f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\Admin\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\Admin\\miniconda3\\envs\\data_process\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5066ef3a-05c0-4f45-a05e-7da52ee7c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练SVM分类器\n",
    "def SVM_Train(data_vec,labels):\n",
    "    #设置SVM模型参数\n",
    "    clf = svm.SVC(decision_function_shape='ovo')\n",
    "    #利用x_train,y_train训练SVM分类器，获得参数\n",
    "    clf.fit(data_vec,labels)\n",
    "    joblib.dump(clf, \"e:/flowers/svm/svm_model.m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9e1f5fe-b6f0-4acc-bc3f-9040a997c5dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train the svm model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m SVM_Train(\u001b[43mdata_vec\u001b[49m,y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_vec' is not defined"
     ]
    }
   ],
   "source": [
    "# train the svm model\n",
    "SVM_Train(data_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd992c8-405c-4e58-ac7c-ff0c56d37576",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_path = 'e:/flowers/train'\n",
    "    test_path = 'e:/flowers/test'\n",
    "    #A建立词袋\n",
    "    build_center(train_path)\n",
    "    #构建训练集特征向量\n",
    "    data_vec,labels = cal_vec(train_path)\n",
    "    #将特征向量和标签输入到SVM分类器中\n",
    "    SVM_Train(data_vec,y_train)\n",
    "    print(x_train.shape)\n",
    "    print(y_train)\n",
    "    #计算测试集的正确率\n",
    "    acc,res = SVM_Test(test_path)\n",
    "    print(acc)\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
